#!/usr/bin/env python3
"""
Unit tests for v3.5.5 features: Report Review & Pipeline Suggestions

Tests cover:
- review_report.py: Report parsing, review header generation, suggestions format
- run_analysis.py: New CLI arguments for review step
"""

import json
import sys
import tempfile
from pathlib import Path

# Add tools directory to path
sys.path.insert(0, str(Path(__file__).parent))

from review_report import (
    build_review_prompt,
    render_suggestions_markdown,
    extract_json_from_response,
)


def create_mock_report() -> str:
    """Create a mock executive summary report for testing."""
    return """# Vacatia AI Voice Agent Performance Report
**Generated:** 2026-01-19 09:23:28 | **Calls Analyzed:** 100

## Executive Summary

The voice agent is failing 66.6% of calls, primarily due to policy gaps.

## Key Metrics at a Glance

| Metric | Value | Assessment | Context |
|--------|-------|------------|---------|
| Success Rate | 33.4% | ❌ | Driven by high volume of unfulfillable intents |
| Escalation Rate | 22.8% | ⚠️ | Driven by lack of self-service tools |
| Failure Rate | 66.6% | ❌ | Driven by dead-end transfers |

## Why Calls Are Failing

**Primary Driver:** Dead-End Escalation Logic

**Contributing Factors:**
- Lack of Hours-of-Operation Awareness
- Rigid Verification Sequencing

## Recommendations

### P0 - Critical

**Implement 'Hours Awareness' check at intent detection**
- Category: process
- Expected Impact: Prevent ~18% of failures
- Example calls: a5e3b044, a6c5e414

---
*Report generated by Vacatia AI Voice Agent Analytics Framework v3.5*
"""


def create_mock_suggestions() -> dict:
    """Create mock pipeline suggestions for testing."""
    return {
        "summary": "Key opportunities exist in temporal analysis and customer journey tracking.",
        "metrics_to_add": [
            {
                "metric": "Call Duration by Outcome",
                "rationale": "Understand time cost of failures vs. successes",
                "data_source": "Already in transcripts (start/end timestamps)"
            },
            {
                "metric": "Repeat Caller Rate",
                "rationale": "Measure true resolution effectiveness",
                "data_source": "Would need caller ID linkage across calls"
            }
        ],
        "insights_to_add": [
            {
                "insight": "Temporal Patterns",
                "rationale": "Are certain failures more common at specific times?",
                "approach": "Add time_of_day and day_of_week to analysis schema"
            }
        ],
        "report_structure": [
            {
                "change": "Add 'Quick Wins' section",
                "rationale": "Separate high-impact, low-effort fixes"
            }
        ],
        "data_capture": [
            {
                "field": "conversation_turns",
                "rationale": "Proxy for complexity",
                "where": "analyze_transcript.py"
            }
        ]
    }


def create_mock_review_response() -> dict:
    """Create mock LLM review response for testing."""
    return {
        "refined_report": """---
## Report Review Summary

**Reviewed:** 2026-01-19 14:35 | **Model:** Gemini 3 Pro

### Refinements Made
- Consolidated redundant mentions of policy gaps
- Improved flow between failure analysis and recommendations

### Data Quality Notes
- Cross-correlation counts are estimates

---

# Vacatia AI Voice Agent Performance Report
**Generated:** 2026-01-19 09:23:28 | **Calls Analyzed:** 100

## Executive Summary

The voice agent is failing 66.6% of calls due to policy gaps and dead-end escalation logic.

[... rest of refined report ...]
""",
        "pipeline_suggestions": create_mock_suggestions()
    }


def test_build_review_prompt_includes_report():
    """Review prompt should include the full report content."""
    report = create_mock_report()
    prompt, appendix = build_review_prompt(report, include_suggestions=True)

    # Should contain the report content
    assert "Vacatia AI Voice Agent Performance Report" in prompt
    assert "66.6%" in prompt
    assert "Dead-End Escalation Logic" in prompt

    # Should contain instructions
    assert "refined_report" in prompt
    assert "pipeline_suggestions" in prompt

    print("  [PASS] test_build_review_prompt_includes_report")


def test_build_review_prompt_no_suggestions():
    """Review prompt should work without suggestions request."""
    report = create_mock_report()
    prompt, appendix = build_review_prompt(report, include_suggestions=False)

    # Should still have refined_report
    assert "refined_report" in prompt

    # Should indicate suggestions disabled
    assert "suggestions disabled" in prompt.lower() or "empty object" in prompt.lower()

    print("  [PASS] test_build_review_prompt_no_suggestions")


def test_render_suggestions_markdown():
    """Pipeline suggestions should render to valid markdown."""
    suggestions = create_mock_suggestions()
    md = render_suggestions_markdown(
        suggestions,
        "executive_summary_v3_20260119.md",
        "2026-01-19 14:35"
    )

    # Should have header
    assert "# Pipeline Improvement Suggestions" in md

    # Should have metadata
    assert "2026-01-19 14:35" in md
    assert "executive_summary_v3_20260119.md" in md

    # Should have summary
    assert "temporal analysis" in md.lower()

    # Should have metrics section
    assert "## Recommended Metrics to Add" in md
    assert "Call Duration by Outcome" in md

    # Should have insights section
    assert "## Recommended Insights to Add" in md
    assert "Temporal Patterns" in md

    # Should have structure section
    assert "## Report Structure Suggestions" in md
    assert "Quick Wins" in md

    # Should have data capture section
    assert "## Data Capture Improvements" in md
    assert "conversation_turns" in md

    print("  [PASS] test_render_suggestions_markdown")


def test_render_suggestions_empty():
    """Empty suggestions should render without errors."""
    suggestions = {"summary": "", "metrics_to_add": [], "insights_to_add": [], "report_structure": [], "data_capture": []}
    md = render_suggestions_markdown(
        suggestions,
        "test.md",
        "2026-01-19"
    )

    # Should still have header
    assert "# Pipeline Improvement Suggestions" in md

    # Should not have empty sections (or handle them gracefully)
    # The sections just won't appear if empty
    assert "Error" not in md

    print("  [PASS] test_render_suggestions_empty")


def test_extract_json_from_code_block():
    """Should extract JSON from markdown code blocks."""
    text = """```json
{
  "refined_report": "# Test Report",
  "pipeline_suggestions": {}
}
```"""

    result = extract_json_from_response(text)
    assert result["refined_report"] == "# Test Report"
    assert result["pipeline_suggestions"] == {}

    print("  [PASS] test_extract_json_from_code_block")


def test_extract_json_raw():
    """Should extract raw JSON without code blocks."""
    text = '{"refined_report": "# Test", "pipeline_suggestions": {}}'

    result = extract_json_from_response(text)
    assert result["refined_report"] == "# Test"

    print("  [PASS] test_extract_json_raw")


def test_extract_json_with_surrounding_text():
    """Should extract JSON from text with surrounding content."""
    text = """Here is the result:

{
  "refined_report": "# Report",
  "pipeline_suggestions": {"summary": "test"}
}

Done!"""

    result = extract_json_from_response(text)
    assert result["refined_report"] == "# Report"
    assert result["pipeline_suggestions"]["summary"] == "test"

    print("  [PASS] test_extract_json_with_surrounding_text")


def test_review_response_has_required_fields():
    """Mock review response should have required fields."""
    response = create_mock_review_response()

    # Should have refined_report
    assert "refined_report" in response
    assert len(response["refined_report"]) > 0

    # refined_report should have review header
    assert "Report Review Summary" in response["refined_report"]
    assert "Refinements Made" in response["refined_report"]

    # Should have pipeline_suggestions
    assert "pipeline_suggestions" in response
    suggestions = response["pipeline_suggestions"]
    assert "summary" in suggestions
    assert "metrics_to_add" in suggestions
    assert "insights_to_add" in suggestions

    print("  [PASS] test_review_response_has_required_fields")


def test_reviewed_report_preserves_numbers():
    """Review header should be prepended but numbers preserved."""
    response = create_mock_review_response()
    refined = response["refined_report"]

    # Should have review header
    assert "## Report Review Summary" in refined

    # Should preserve original numbers
    assert "66.6%" in refined or "100" in refined  # Original data preserved

    print("  [PASS] test_reviewed_report_preserves_numbers")


def test_suggestions_has_required_sections():
    """Suggestions should have all required sections."""
    suggestions = create_mock_suggestions()

    required_keys = ["summary", "metrics_to_add", "insights_to_add", "report_structure", "data_capture"]
    for key in required_keys:
        assert key in suggestions, f"Missing required key: {key}"

    # metrics_to_add should have required fields
    for metric in suggestions["metrics_to_add"]:
        assert "metric" in metric
        assert "rationale" in metric
        assert "data_source" in metric

    # insights_to_add should have required fields
    for insight in suggestions["insights_to_add"]:
        assert "insight" in insight
        assert "rationale" in insight
        assert "approach" in insight

    print("  [PASS] test_suggestions_has_required_sections")


def test_cli_arguments_exist():
    """run_analysis.py should have review-related CLI arguments."""
    run_analysis_path = Path(__file__).parent / "run_analysis.py"
    content = run_analysis_path.read_text()

    # Check for new arguments
    assert "--skip-review" in content, "Missing --skip-review argument"
    assert "--review-model" in content, "Missing --review-model argument"
    assert "--no-suggestions" in content, "Missing --no-suggestions argument"

    # Check for Step 7
    assert "Step 7" in content or "review_report.py" in content, "Missing Step 7 review step"

    print("  [PASS] test_cli_arguments_exist")


def run_all_tests():
    """Run all v3.5.5 feature tests."""
    print("\n" + "=" * 60)
    print("v3.5.5 FEATURE TESTS")
    print("=" * 60)

    tests = [
        test_build_review_prompt_includes_report,
        test_build_review_prompt_no_suggestions,
        test_render_suggestions_markdown,
        test_render_suggestions_empty,
        test_extract_json_from_code_block,
        test_extract_json_raw,
        test_extract_json_with_surrounding_text,
        test_review_response_has_required_fields,
        test_reviewed_report_preserves_numbers,
        test_suggestions_has_required_sections,
        test_cli_arguments_exist,
    ]

    passed = 0
    failed = 0

    for test_func in tests:
        try:
            test_func()
            passed += 1
        except AssertionError as e:
            print(f"  [FAIL] {test_func.__name__}: {e}")
            failed += 1
        except Exception as e:
            print(f"  [ERROR] {test_func.__name__}: {e}")
            failed += 1

    print("\n" + "-" * 40)
    print(f"RESULTS: {passed} passed, {failed} failed")
    print("=" * 60)

    return 0 if failed == 0 else 1


if __name__ == "__main__":
    exit(run_all_tests())
